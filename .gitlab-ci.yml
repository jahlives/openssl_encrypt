default:
  image: python:3.13-alpine
  cache:
    paths:
      - .pip-cache/
  before_script:
    - python3 --version
    - pip install --upgrade pip --break-system-packages
    - pip install build twine PyYAML --break-system-packages

stages:
  - docker
  - sync
  - security
  - prepare
  - test
  - build
  - publish

variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.pip-cache"
  DOCKER_IMAGE: $CI_REGISTRY_IMAGE/python-liboqs:3.13-alpine
  
# This job builds a custom Docker image with liboqs pre-installed
# It runs on a schedule (e.g., daily) to keep the image updated
build-docker-image:
  stage: docker
  image: docker:latest
  services:
    - name: docker:dind
      alias: docker
  variables:
    # Try with TLS disabled first for troubleshooting
    DOCKER_HOST: tcp://docker:2375
    DOCKER_TLS_CERTDIR: ""
  before_script:
    # Create Docker context for diagnostics
    - mkdir -p ~/.docker/cli-plugins/
    # Explicitly set Docker host for diagnostics
    - export DOCKER_HOST=tcp://docker:2375
    # Diagnostics (but don't fail pipeline if they fail)
    - docker info || true
    - docker version || true
    # Log in to GitLab registry
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    # Test Docker connectivity before proceeding
    - docker ps
    - |
      cat > Dockerfile <<EOF
      FROM python:3.13-alpine
      
      # Install dependencies
      RUN apk add --no-cache git gcc g++ cmake ninja make go python3-dev openssl-dev musl-dev
      
      # Clone and build liboqs
      RUN git clone --recurse-submodules https://github.com/open-quantum-safe/liboqs.git && \
          cd liboqs && \
          mkdir build && cd build && \
          cmake -GNinja -DCMAKE_INSTALL_PREFIX=/usr/local .. && \
          ninja && \
          ninja install && \
          cd / && \
          rm -rf liboqs
      
      # Install liboqs Python bindings
      RUN pip install --no-cache-dir git+https://github.com/open-quantum-safe/liboqs-python.git
      
      # Install other common dependencies
      RUN pip install --no-cache-dir pytest pytest-order
      EOF
    - docker build -t $DOCKER_IMAGE .
    - docker push $DOCKER_IMAGE
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
    - if: $CI_COMMIT_BRANCH == "dev" && $BUILD_DOCKER_IMAGE == "true"
      when: manual
      allow_failure: true


sync-wiki:
  stage: sync
  image: alpine:latest
  before_script:
    - apk add --no-cache git
  script:
    - git clone https://oauth2:${PROJECT_ACCESS_TOKEN}@${CI_SERVER_HOST}/${CI_PROJECT_PATH}.wiki.git wiki
    - cp -r openssl_encrypt/docs/* wiki/
    - cd wiki
    - git config --global user.email "pipeline@rm-rf.ch"
    - git config --global user.name "GitLab Pipeline"
    - git add .
    - git diff-index --quiet HEAD || git commit -m "Update wiki from docs folder"
    - git push
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "release"
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH == "dev"
      when: manual
      allow_failure: true

# Dependency security scanning
dependency-scan:
  stage: security
  image: python:3.13
  script:
    - pip install pip-audit bandit
    # Use custom script that handles dependency scanning and GitLab report generation
    - python scripts/gitlab_dependency_scan.py
    # Display results for visibility in logs
    - echo "=== Production Dependencies Scan ==="
    - cat pip-audit-prod-results.json || echo "No production scan results available"
    - echo "=== Development Dependencies Scan ==="
    - cat pip-audit-dev-results.json || echo "No development scan results available"
    - echo "=== GitLab Dependency Scanning Report ==="
    - cat gl-dependency-scanning-report.json || echo "No report available"
  artifacts:
    reports:
      dependency_scanning: gl-dependency-scanning-report.json
    paths:
      - gl-dependency-scanning-report.json
      - pip-audit-prod-results.json
      - pip-audit-dev-results.json
    expire_in: 1 week
  rules:
    - if: $CI_COMMIT_BRANCH == "dev"
    - if: $CI_COMMIT_BRANCH == "testing"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "release"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"

# Python code security scanning
code-security-scan:
  stage: security
  image: python:3.13
  before_script:
    - pip install bandit
  script:
    # Run bandit but don't fail the job if security issues are found
    - bandit -r openssl_encrypt/ -c .bandit.yaml -f json -o gl-sast-report.json || true
    - bandit -r openssl_encrypt/ -c .bandit.yaml -f txt || true  # Display results in terminal but don't fail
  artifacts:
    reports:
      sast: gl-sast-report.json
    paths:
      - gl-sast-report.json
    expire_in: 1 week
  # Always succeed even if security issues are found
  allow_failure: true
  rules:
    - if: $CI_COMMIT_BRANCH == "dev"
    - if: $CI_COMMIT_BRANCH == "testing"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "release"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    
# Software Bill of Materials generation
sbom-generation:
  stage: security
  image: python:3.13
  before_script:
    - pip install cyclonedx-bom
  script:
    # cyclonedx-py command format has changed, needs to be called with requirements subcommand
    - cyclonedx-py requirements -r requirements-prod.txt -o bom.json
    - python -c "import json; f = open('bom.json', 'r'); data = json.load(f); f.close(); print(json.dumps(data, indent=2))" | head -n 50  # Display first 50 lines as a preview
  artifacts:
    paths:
      - bom.json
    expire_in: 1 week
  rules:
    - if: $CI_COMMIT_BRANCH == "dev"
    - if: $CI_COMMIT_BRANCH == "testing"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "release"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"

test:
  stage: test
  image: $DOCKER_IMAGE
  variables:
    PQC_QUIET: "true"  # Suppress PQC module initialization messages
  before_script:
    - pip install -r requirements.txt --break-system-packages
    - pip install setuptools --break-system-packages  # Provides pkg_resources
  script:
    - python3 -m pytest openssl_encrypt/unittests/unittests.py -v
  rules:
    # Run tests on dev, testing, and release branches, and for tags
    - if: $CI_COMMIT_BRANCH == "dev"
    - if: $CI_COMMIT_BRANCH == "testing"
    - if: $CI_COMMIT_BRANCH == "release"
      # Must succeed on release branch
      allow_failure: false
    - if: $CI_COMMIT_TAG
  # Fallback configuration if the custom image is not available
  # This ensures the pipeline still works if the image hasn't been built yet
  retry:
    max: 2
    when:
      - runner_system_failure
      - script_failure
  
# Fallback test job that builds liboqs from scratch
# This runs only if the main test job fails, typically due to missing image
test-fallback:
  stage: test
  image: python:3.13-alpine
  variables:
    PQC_QUIET: "true"  # Suppress PQC module initialization messages
  before_script:
    - pip install -r requirements.txt --break-system-packages
    - pip install pytest pytest-order setuptools --break-system-packages  # setuptools provides pkg_resources
    # Explicitly pin cryptography version to match the tests
    # Install dependencies for building liboqs (Alpine Linux packages)
    - apk add git gcc g++ cmake ninja make go python3-dev openssl-dev musl-dev
    # Clone liboqs repository with submodules
    - git clone --recurse-submodules https://github.com/open-quantum-safe/liboqs.git
    # Build and install liboqs
    - cd liboqs
    - mkdir build && cd build
    - cmake -GNinja -DCMAKE_INSTALL_PREFIX=/usr/local ..
    - ninja
    - ninja install
    # Return to project root directory
    - cd $CI_PROJECT_DIR
    # Install liboqs Python bindings
    - pip install --user git+https://github.com/open-quantum-safe/liboqs-python.git
  script:
    - python3 -m pytest openssl_encrypt/unittests/unittests.py -v
  rules:
    - if: $CI_COMMIT_BRANCH == "dev"
      when: on_failure
    - if: $CI_COMMIT_BRANCH == "testing"
      when: on_failure
    - if: $CI_COMMIT_BRANCH == "release"
      when: on_failure
      # Must succeed on release branch
      allow_failure: false
    - if: $CI_COMMIT_TAG
      when: on_failure


build:
  stage: build
  needs:
    - job: test
      optional: true
  script:
    - python3 -m build
  artifacts:
    paths:
      - dist/
  rules:
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "release"
    - if: $CI_COMMIT_BRANCH == "testing"
    - if: $CI_COMMIT_BRANCH == "dev"


publish:
  stage: publish
  needs:
    - job: build
      artifacts: true
  script:
    - pip install --upgrade twine --break-system-packages
    - echo "Attempting to publish with CI_JOB_TOKEN..."
    - TWINE_PASSWORD=${PROJECT_ACCESS_TOKEN} TWINE_USERNAME=oauth2 python3 -m twine upload --verbose --repository-url "https://${MY_GITLAB_PYPI}/api/v4/projects/2/packages/pypi" dist/*
  rules:
    - if: $CI_COMMIT_TAG
      when: on_success
    - if: $CI_COMMIT_BRANCH == "release"
      when: manual
      allow_failure: true

publish-pypi-test:
  stage: publish
  image: python:3.13-alpine
  needs:
    - job: build
      artifacts: true
  script:
    - pip install build twine --break-system-packages
    - TWINE_PASSWORD=${PYPI_API_TOKEN} TWINE_USERNAME=__token__ python3 -m twine upload --repository-url https://test.pypi.org/legacy/ dist/*
  rules:
    - if: $CI_COMMIT_TAG
      when: on_success
    - if: $CI_COMMIT_BRANCH == "release"
      when: manual
      allow_failure: true

publish-pypi-prod:
  stage: publish
  image: python:3.13-alpine
  needs:
    - job: build
      artifacts: true
  script:
    - pip install build twine --break-system-packages
    - TWINE_PASSWORD=${PYPI_API_TOKEN_PROD} TWINE_USERNAME=__token__ python3 -m twine upload dist/*
  rules:
    - if: $CI_COMMIT_TAG
      when: on_success
    - if: $CI_COMMIT_BRANCH == "release"
      when: manual
      allow_failure: true
